{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9838c6-447e-430f-9cda-5f38fe676eab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Product  ...                                 Combined_Labels\n0      Honey Treasures Leave-In Miracle Nectar Treatment  ...  Hair Care|||Hair Treat and Protect|||Treatment\n1         Eva NYC Therapy Session Hair Mask - 16.9 fl oz  ...       Hair Care|||Hair Treat and Protect|||Mask\n2             Eva NYC Therapy Session Hair Mask, 16.9 OZ  ...       Hair Care|||Hair Treat and Protect|||Mask\n3      Kitsch Rice Water Protein Strengthening Shampo...  ...                   Hair Care|||Shampoo|||Shampoo\n4      One Signature Conditioner - Moisturizes, Smoot...  ...           Hair Care|||Conditioner|||Conditioner\n...                                                  ...  ...                                             ...\n12655  Cremo Hair Sculpting Clay, High Hold, Matte Fi...  ...     Hair Care|||Hair Styling|||Styling Products\n12656      ($24 Value) Aquage Transforming Paste, 4.6 oz  ...     Hair Care|||Hair Styling|||Styling Products\n12657                         GIBS Tea Tree Pomade, 4 oz  ...     Hair Care|||Hair Styling|||Styling Products\n12658  First Active Straightening Shampoo Kit Keratin...  ...           Hair Care|||Combo Packs|||Combo Packs\n12659  Newvenper Dry Shampoo for All Hair Types Dry S...  ...                   Hair Care|||Shampoo|||Shampoo\n\n[12660 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Workspace/Users/anmol@i-genie.ai/MDL_Coding/Hair_Care_Corrected.csv\")\n",
    "df.head()\n",
    "\n",
    "# Replace None and empty strings with a placeholder 'None'\n",
    "df.fillna('None', inplace=True)\n",
    "df.replace('', 'None', inplace=True)\n",
    "\n",
    "# Concatenate the output columns\n",
    "df['Combined_Labels'] = df['Category'] + '|||' + df['Segment'] + '|||' + df['Sub-Segment']\n",
    "\n",
    "print(df[['Product', 'Product Category', 'Combined_Labels']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cfd438-5cb1-4f1b-a5d0-641210f37b05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os, pickle\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class RoBERTaClassifier:\n",
    "    def __init__(self, model_name='roberta-base', max_len=128, model_path=\"/dbfs/mnt/igenie-blob01/Anmol_AI_dir/MDP/\", category=\"product_type\"):\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.max_len = max_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.product_model_path = os.path.join(model_path, category)\n",
    "\n",
    "    class TextClassificationDataset(Dataset):\n",
    "        def __init__(self, texts, labels, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_attention_mask=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "    def prepare_data(self, df, input_columns, label_column):\n",
    "        df['input_text'] = df[input_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        texts = df['input_text'].tolist()\n",
    "        labels = df[label_column].tolist()\n",
    "        \n",
    "        self.label_encoder = LabelEncoder()\n",
    "        encoded_labels = self.label_encoder.fit_transform(labels)\n",
    "        num_classes = len(set(encoded_labels))\n",
    "        \n",
    "        return texts, encoded_labels, num_classes\n",
    "\n",
    "    def evaluate_model(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_eval_loss = 0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(input_ids=batch['input_ids'],\n",
    "                                    attention_mask=batch['attention_mask'],\n",
    "                                    labels=batch['labels'])\n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(val_loader)\n",
    "        overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        metrics_report = self.enhancedClassificationReport(all_labels, all_predictions)\n",
    "\n",
    "        return {\n",
    "            'avg_val_loss': avg_val_loss,\n",
    "            'classification_report': metrics_report,\n",
    "            'accuracy': overall_accuracy\n",
    "        }\n",
    "\n",
    "    def enhancedClassificationReport(self, true_labels, predicted_labels):\n",
    "            report = classification_report(true_labels, predicted_labels, output_dict=True, zero_division=0)\n",
    "            cm = confusion_matrix(true_labels, predicted_labels)\n",
    "            TP = cm.diagonal()\n",
    "            FP = cm.sum(axis=0) - TP\n",
    "            FN = cm.sum(axis=1) - TP\n",
    "            \n",
    "            # Enhance the report with TP, FP, FN\n",
    "            for i, label in enumerate(self.label_encoder.classes_):\n",
    "                report[str(i)]['TP'] = TP[i]\n",
    "                report[str(i)]['FP'] = FP[i]\n",
    "                report[str(i)]['FN'] = FN[i]\n",
    "            return report\n",
    "        \n",
    "    def get_label_name(self, label):\n",
    "        try:\n",
    "            # Attempt to transform the label using the encoder\n",
    "            if any([label, int(label) in self.label_encoder.transform(self.label_encoder.classes_)]):\n",
    "                return self.label_encoder.inverse_transform([int(label)])[0]\n",
    "            else:\n",
    "                return label\n",
    "        except ValueError:\n",
    "            # If the label is not recognized by the encoder, return it as is\n",
    "            return label\n",
    "    \n",
    "    def comparativePivot(self, metrics_df):\n",
    "\n",
    "        # Define your preferred order of metrics\n",
    "        metrics_values = ['F1-Score', 'Precision', 'Recall', 'Support', 'TP', 'FP', 'FN']\n",
    "\n",
    "        pivot_df = metrics_df.pivot_table(\n",
    "            index=['Label', 'Label Name'],\n",
    "            columns=['Phase', 'Epoch'],\n",
    "            values=metrics_values,\n",
    "            aggfunc='first'\n",
    "        ).reset_index()\n",
    "\n",
    "        # Ensure the DataFrame's columns are lexsorted\n",
    "        df = pivot_df.sort_index(axis=1)\n",
    "\n",
    "        # Sorting based on test F1-scores\n",
    "        special_rows = df[df['Label Name'].isin(['weighted avg', 'macro avg'])]\n",
    "        df_rest = df[~df['Label Name'].isin(['weighted avg', 'macro avg'])]\n",
    "        max_index = df_rest['F1-Score', 'test-set'].columns.max()\n",
    "        df_rest_sorted = df_rest.sort_values(by=('F1-Score', 'test-set', max_index), ascending=False)\n",
    "        pivot_df = pd.concat([df_rest_sorted, special_rows])\n",
    "        pivot_df.set_index(['Label', 'Label Name'], inplace=True)\n",
    "\n",
    "        # sorting column indexes & formatting values\n",
    "        sorted_columns = sorted(pivot_df.columns, key=lambda x: (metrics_values.index(x[0]), x[1], x[2]))\n",
    "        pivot_df = pivot_df[sorted_columns]\n",
    "        pivot_df.update(pivot_df[['F1-Score', 'Precision', 'Recall']].apply(lambda x: x * 100).round(2).astype(str) + '%')\n",
    "        \n",
    "        return pivot_df\n",
    "    \n",
    "    def build_metrics_dataframe(self, all_epoch_metrics, test_metrics=None, phase_name='test-set'):\n",
    "        epochs = len(all_epoch_metrics)\n",
    "        data = []\n",
    "\n",
    "        # Collecting data for each epoch\n",
    "        for epoch, metrics in enumerate(all_epoch_metrics, start=1):\n",
    "            for label, scores in metrics['classification_report'].items():\n",
    "                label_name = self.get_label_name(label)\n",
    "                if isinstance(scores, dict): \n",
    "                    data.append({\n",
    "                        'Phase': phase_name,\n",
    "                        'Epoch': epoch,\n",
    "                        'Label': label,\n",
    "                        'Label Name': label_name,\n",
    "                        'Precision': scores.get('precision'),\n",
    "                        'Recall': scores.get('recall'),\n",
    "                        'F1-Score': scores.get('f1-score'),\n",
    "                        'Support': scores.get('support'),\n",
    "                        'TP': scores.get('TP'),\n",
    "                        'FP': scores.get('FP'),\n",
    "                        'FN': scores.get('FN')\n",
    "                    })\n",
    "\n",
    "        # Adding test metrics\n",
    "        if test_metrics:\n",
    "            for label, scores in test_metrics['classification_report'].items():\n",
    "                label_name = self.get_label_name(label)\n",
    "                if isinstance(scores, dict):\n",
    "                    data.append({\n",
    "                        'Phase': 'test-set',\n",
    "                        'Epoch': epoch+1,\n",
    "                        'Label': label,\n",
    "                        'Label Name': label_name,\n",
    "                        'Precision': scores.get('precision'),\n",
    "                        'Recall': scores.get('recall'),\n",
    "                        'F1-Score': scores.get('f1-score'),\n",
    "                        'Support': scores.get('support'),\n",
    "                        'TP': scores.get('TP'),\n",
    "                        'FP': scores.get('FP'),\n",
    "                        'FN': scores.get('FN')\n",
    "                    })\n",
    "\n",
    "        metrics_df = pd.DataFrame(data)\n",
    "        return metrics_df, self.comparativePivot(metrics_df)\n",
    " \n",
    "    def plotMetrics(self, metrics_df):\n",
    "        # Define the metrics you want to plot\n",
    "        metrics = ['F1-Score', 'Precision', 'Recall'] \n",
    "\n",
    "        # Iterate over each metric to create a separate plot\n",
    "        for metric in metrics:\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # Group by label to plot each Metric\n",
    "            for (label, label_name), group in metrics_df.groupby(['Label', 'Label Name']):\n",
    "\n",
    "                # Format label based on its type (numeric or string)\n",
    "                formatted_label = f'{int(label):02}' if str(label).isdigit() else label\n",
    "                # Construct the trace name\n",
    "                trace_name = f'{formatted_label} - {label_name}'\n",
    "\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=group['Epoch'],\n",
    "                    y=group[metric],\n",
    "                    mode='lines+markers',\n",
    "                    name=trace_name\n",
    "                ))\n",
    "\n",
    "            # Update plot layout\n",
    "            fig.update_layout(\n",
    "                title=f'{metric} Over Epochs',\n",
    "                xaxis_title='Epochs',\n",
    "                yaxis_title=metric,\n",
    "                legend_title='Label',\n",
    "                hovermode='x unified'\n",
    "            )\n",
    "\n",
    "            # Show the plot\n",
    "            fig.show()\n",
    "\n",
    "    def save_checkpoint(self, model, label_encoder):\n",
    "        if not os.path.exists(self.product_model_path):\n",
    "            os.makedirs(self.product_model_path)\n",
    "        torch.save(model.state_dict(), os.path.join(self.product_model_path, 'roberta_model.pt'))\n",
    "        with open(os.path.join(self.product_model_path, 'label_encoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(label_encoder, f)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        with open(os.path.join(self.product_model_path, 'label_encoder.pkl'), 'rb') as f:\n",
    "            label_encoder = pickle.load(f)\n",
    "\n",
    "        model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n",
    "        # Load the model state dict with DataParallel wrapper\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        model.load_state_dict(torch.load(os.path.join(self.product_model_path, 'roberta_model.pt')))\n",
    "        model = model.module.to(self.device)  # Remove DataParallel wrapper after loading\n",
    "        model.eval()\n",
    "\n",
    "        return model, label_encoder\n",
    "\n",
    "    def train_and_evaluate(self, df, input_columns, label_column, batch_size=64, epochs=3, learning_rate=2e-5):\n",
    "        texts, labels, num_classes = self.prepare_data(df, input_columns, label_column)\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_classes)\n",
    "        self.model = DataParallel(self.model)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # First, split into training+validation and test sets\n",
    "        train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(\n",
    "            texts, labels, test_size=0.1, random_state=42, stratify=labels)\n",
    "\n",
    "        # Then, split the remaining data into training and validation sets\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            train_val_texts, train_val_labels, test_size=0.1, random_state=42, stratify=train_val_labels)  \n",
    "        \n",
    "        train_dataset = self.TextClassificationDataset(train_texts, train_labels, self.tokenizer, self.max_len)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataset = self.TextClassificationDataset(val_texts, val_labels, self.tokenizer, self.max_len)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        all_epoch_metrics = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            train_progress = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}, Training')\n",
    "            total_train_loss = 0\n",
    "            for batch in train_progress:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(input_ids=batch['input_ids'].to(self.device),\n",
    "                                     attention_mask=batch['attention_mask'].to(self.device),\n",
    "                                     labels=batch['labels'].to(self.device))\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                train_progress.set_postfix(loss=loss.item())\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            epoch_metrics = self.evaluate_model(val_loader)\n",
    "            all_epoch_metrics.append(epoch_metrics)\n",
    "\n",
    "            print(f\"\\nEpoch {epoch + 1} complete. Training Loss: {avg_train_loss}, Validation Loss: {epoch_metrics.get('avg_val_loss')}, Validation Accuracy: {epoch_metrics.get('accuracy')}\")\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        test_dataset = self.TextClassificationDataset(test_texts, test_labels, self.tokenizer, self.max_len)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_metrics = self.evaluate_model(test_loader)\n",
    "\n",
    "        # Comparative Analysis & Metrics Visualization\n",
    "        comparative_metrics_analysis_df, pivot_metrics_df = self.build_metrics_dataframe(all_epoch_metrics, test_metrics, phase_name='dev-set')\n",
    "        self.plotMetrics(comparative_metrics_analysis_df)\n",
    "\n",
    "        # Saving checkpoints\n",
    "        self.save_checkpoint(self.model, self.label_encoder)\n",
    "        print(\"Saved best model checkpoint.\")\n",
    "        return comparative_metrics_analysis_df, pivot_metrics_df\n",
    "\n",
    "    def predict(self, df, input_columns, label_column=None, use_checkpoint=True):\n",
    "        if use_checkpoint:\n",
    "            self.model, self.label_encoder = self.load_checkpoint()\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "\n",
    "        # Prepare input text from the dataframe using specified input columns\n",
    "        df['input_text'] = df[input_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        texts = df['input_text'].tolist()\n",
    "\n",
    "        prediction_progress = tqdm(texts, desc='Predicting')\n",
    "        for text in prediction_progress:\n",
    "            encoded_input = self.tokenizer.encode_plus(\n",
    "                text, \n",
    "                add_special_tokens=True, \n",
    "                max_length=self.max_len, \n",
    "                return_attention_mask=True,\n",
    "                padding='max_length', \n",
    "                truncation=True, \n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(\n",
    "                    input_ids=encoded_input['input_ids'].to(self.device),\n",
    "                    attention_mask=encoded_input['attention_mask'].to(self.device)\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                predicted_label_indices = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "                predicted_labels = self.label_encoder.inverse_transform(predicted_label_indices)\n",
    "                predictions.extend(predicted_labels)\n",
    "\n",
    "        # Add predictions to DataFrame\n",
    "        df['Predicted'] = predictions\n",
    "        predict_report = pd.DataFrame()\n",
    "\n",
    "        # If a label column for evaluation is provided, calculate additional metrics\n",
    "        if label_column is not None:\n",
    "            true_labels = df[label_column].tolist()\n",
    "            encoded_true_labels = self.label_encoder.transform(true_labels)\n",
    "            enhanced_report = self.enhancedClassificationReport(encoded_true_labels, self.label_encoder.transform(predictions))\n",
    "            predict_report = pd.DataFrame(enhanced_report).T\n",
    "            label_name = predict_report.apply(lambda x: self.get_label_name(x.name), axis=1)\n",
    "            predict_report.insert(loc=0, column='label_name', value=label_name)\n",
    "            predict_report.update(predict_report[['f1-score', 'precision', 'recall']].apply(lambda x: x * 100).round(2).astype(str) + '%')\n",
    "\n",
    "            # Sorting based on test F1-scores\n",
    "            special_rows = predict_report[predict_report['label_name'].isin(['accuracy', 'weighted avg', 'macro avg'])]\n",
    "            df_rest = predict_report[~predict_report['label_name'].isin(['accuracy', 'weighted avg', 'macro avg'])]\n",
    "            df_rest_sorted = df_rest.sort_values(by='f1-score', ascending=False)\n",
    "            predict_report = pd.concat([df_rest_sorted, special_rows])\n",
    "        return df, predict_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65008ce6-097f-472c-90d2-f08212b2c44f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# classifier starts\n",
    "bert_classifier = RoBERTaClassifier(model_path=\"/dbfs/mnt/igenie-blob01/Anmol_AI_dir/MDP/\", category=\"hair_care\")\n",
    "\n",
    "# # Train the model\n",
    "# comparative_metrics_analysis_df, pivot_metrics_df= bert_classifier.train_and_evaluate(df, \n",
    "#                                                                                       input_columns=['Product', 'Product Category'], \n",
    "#                                                                                       label_column='Combined_Labels', \n",
    "#                                                                                       batch_size=128, \n",
    "#                                                                                       epochs=2, learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d5291b-7600-4969-bafe-035da46813ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\rPredicting:   0%|          | 0/12660 [00:00<?, ?it/s]\rPredicting:   0%|          | 1/12660 [00:00<1:32:55,  2.27it/s]\rPredicting:   0%|          | 8/12660 [00:00<11:34, 18.23it/s]  \rPredicting:   0%|          | 17/12660 [00:00<05:55, 35.56it/s]\rPredicting:   0%|          | 26/12660 [00:00<04:14, 49.57it/s]\rPredicting:   0%|          | 38/12660 [00:00<03:05, 68.20it/s]\rPredicting:   0%|          | 50/12660 [00:00<02:33, 82.03it/s]\rPredicting:   0%|          | 62/12660 [00:01<02:17, 91.85it/s]\rPredicting:   1%|          | 74/12660 [00:01<02:07, 98.97it/s]\rPredicting:   1%|          | 86/12660 [00:01<02:00, 104.36it/s]\rPredicting:   1%|          | 98/12660 [00:01<01:55, 108.51it/s]\rPredicting:   1%|          | 110/12660 [00:01<01:53, 110.99it/s]\rPredicting:   1%|          | 122/12660 [00:01<01:51, 112.84it/s]\rPredicting:   1%|          | 134/12660 [00:01<01:50, 113.22it/s]\rPredicting:   1%|          | 146/12660 [00:01<01:50, 113.65it/s]\rPredicting:   1%|          | 158/12660 [00:01<01:49, 114.40it/s]\rPredicting:   1%|▏         | 170/12660 [00:02<01:48, 115.04it/s]\rPredicting:   1%|▏         | 182/12660 [00:02<01:48, 115.16it/s]\rPredicting:   2%|▏         | 194/12660 [00:02<01:48, 114.95it/s]\rPredicting:   2%|▏         | 206/12660 [00:02<01:47, 115.65it/s]\rPredicting:   2%|▏         | 218/12660 [00:02<01:47, 116.18it/s]\rPredicting:   2%|▏         | 230/12660 [00:02<01:46, 116.73it/s]\rPredicting:   2%|▏         | 242/12660 [00:02<01:46, 116.98it/s]\rPredicting:   2%|▏         | 254/12660 [00:02<01:46, 116.80it/s]\rPredicting:   2%|▏         | 266/12660 [00:02<01:46, 116.79it/s]\rPredicting:   2%|▏         | 278/12660 [00:02<01:45, 116.94it/s]\rPredicting:   2%|▏         | 290/12660 [00:03<01:45, 117.06it/s]\rPredicting:   2%|▏         | 302/12660 [00:03<01:45, 117.27it/s]\rPredicting:   2%|▏         | 314/12660 [00:03<01:45, 117.27it/s]\rPredicting:   3%|▎         | 326/12660 [00:03<01:44, 117.48it/s]\rPredicting:   3%|▎         | 338/12660 [00:03<01:44, 117.83it/s]\rPredicting:   3%|▎         | 350/12660 [00:03<01:44, 118.04it/s]\rPredicting:   3%|▎         | 362/12660 [00:03<01:44, 117.51it/s]\rPredicting:   3%|▎         | 374/12660 [00:03<01:45, 116.93it/s]\rPredicting:   3%|▎         | 386/12660 [00:03<01:44, 117.04it/s]\rPredicting:   3%|▎         | 398/12660 [00:03<01:44, 117.56it/s]\rPredicting:   3%|▎         | 410/12660 [00:04<01:44, 117.65it/s]\rPredicting:   3%|▎         | 422/12660 [00:04<01:43, 117.89it/s]\rPredicting:   3%|▎         | 434/12660 [00:04<01:43, 118.23it/s]\rPredicting:   4%|▎         | 446/12660 [00:04<01:43, 118.34it/s]\rPredicting:   4%|▎         | 458/12660 [00:04<01:43, 118.04it/s]\rPredicting:   4%|▎         | 470/12660 [00:04<01:42, 118.37it/s]\rPredicting:   4%|▍         | 482/12660 [00:04<01:43, 117.94it/s]\rPredicting:   4%|▍         | 494/12660 [00:04<01:43, 117.88it/s]\rPredicting:   4%|▍         | 506/12660 [00:04<01:43, 117.95it/s]\rPredicting:   4%|▍         | 518/12660 [00:04<01:43, 117.54it/s]\rPredicting:   4%|▍         | 530/12660 [00:05<01:43, 117.38it/s]\rPredicting:   4%|▍         | 542/12660 [00:05<01:43, 117.45it/s]\rPredicting:   4%|▍         | 554/12660 [00:05<01:42, 117.54it/s]\rPredicting:   4%|▍         | 566/12660 [00:05<01:42, 117.74it/s]\rPredicting:   5%|▍         | 578/12660 [00:05<01:42, 118.02it/s]\rPredicting:   5%|▍         | 590/12660 [00:05<01:42, 117.77it/s]\rPredicting:   5%|▍         | 602/12660 [00:05<01:42, 117.71it/s]\rPredicting:   5%|▍         | 614/12660 [00:05<01:42, 117.72it/s]\rPredicting:   5%|▍         | 626/12660 [00:05<01:42, 117.53it/s]\rPredicting:   5%|▌         | 638/12660 [00:05<01:42, 117.28it/s]\rPredicting:   5%|▌         | 650/12660 [00:06<01:42, 117.43it/s]\rPredicting:   5%|▌         | 662/12660 [00:06<01:42, 117.58it/s]\rPredicting:   5%|▌         | 674/12660 [00:06<01:42, 117.51it/s]\rPredicting:   5%|▌         | 686/12660 [00:06<01:41, 117.71it/s]\rPredicting:   6%|▌         | 698/12660 [00:06<01:42, 116.44it/s]\rPredicting:   6%|▌         | 710/12660 [00:06<01:42, 116.95it/s]\rPredicting:   6%|▌         | 722/12660 [00:06<01:41, 117.42it/s]\rPredicting:   6%|▌         | 734/12660 [00:06<01:42, 116.89it/s]\rPredicting:   6%|▌         | 746/12660 [00:06<01:45, 113.16it/s]\rPredicting:   6%|▌         | 758/12660 [00:07<01:43, 114.69it/s]\rPredicting:   6%|▌         | 770/12660 [00:07<01:42, 115.48it/s]\rPredicting:   6%|▌         | 782/12660 [00:07<01:43, 114.93it/s]\rPredicting:   6%|▋         | 794/12660 [00:07<01:42, 115.57it/s]\rPredicting:   6%|▋         | 806/12660 [00:07<01:42, 115.62it/s]\rPredicting:   6%|▋         | 818/12660 [00:07<01:42, 116.02it/s]\rPredicting:   7%|▋         | 830/12660 [00:07<01:41, 116.61it/s]\rPredicting:   7%|▋         | 842/12660 [00:07<01:41, 116.91it/s]\rPredicting:   7%|▋         | 854/12660 [00:07<01:40, 117.16it/s]\rPredicting:   7%|▋         | 866/12660 [00:07<01:40, 117.06it/s]\rPredicting:   7%|▋         | 878/12660 [00:08<01:40, 117.08it/s]\rPredicting:   7%|▋         | 890/12660 [00:08<01:40, 117.18it/s]\rPredicting:   7%|▋         | 902/12660 [00:08<01:40, 117.07it/s]\rPredicting:   7%|▋         | 914/12660 [00:08<01:41, 116.17it/s]\rPredicting:   7%|▋         | 926/12660 [00:08<01:41, 116.14it/s]\rPredicting:   7%|▋         | 938/12660 [00:08<01:40, 116.24it/s]\rPredicting:   8%|▊         | 950/12660 [00:08<01:40, 116.16it/s]\rPredicting:   8%|▊         | 962/12660 [00:08<01:40, 115.86it/s]\rPredicting:   8%|▊         | 974/12660 [00:08<01:40, 115.88it/s]\rPredicting:   8%|▊         | 986/12660 [00:08<01:40, 115.99it/s]\rPredicting:   8%|▊         | 998/12660 [00:09<01:40, 116.36it/s]\rPredicting:   8%|▊         | 1010/12660 [00:09<01:40, 116.10it/s]\rPredicting:   8%|▊         | 1022/12660 [00:09<01:39, 116.41it/s]\rPredicting:   8%|▊         | 1034/12660 [00:09<01:39, 116.47it/s]\rPredicting:   8%|▊         | 1046/12660 [00:09<01:39, 116.71it/s]\rPredicting:   8%|▊         | 1058/12660 [00:09<01:39, 116.61it/s]\rPredicting:   8%|▊         | 1070/12660 [00:09<01:39, 116.48it/s]\rPredicting:   9%|▊         | 1082/12660 [00:09<01:39, 116.46it/s]\rPredicting:   9%|▊         | 1094/12660 [00:09<01:39, 116.10it/s]\rPredicting:   9%|▊         | 1106/12660 [00:10<01:39, 116.38it/s]\rPredicting:   9%|▉         | 1118/12660 [00:10<01:39, 116.55it/s]\rPredicting:   9%|▉         | 1130/12660 [00:10<01:39, 115.78it/s]\rPredicting:   9%|▉         | 1142/12660 [00:10<01:40, 115.12it/s]\rPredicting:   9%|▉         | 1154/12660 [00:10<01:39, 115.47it/s]\rPredicting:   9%|▉         | 1166/12660 [00:10<01:39, 115.40it/s]\rPredicting:   9%|▉         | 1178/12660 [00:10<01:39, 115.72it/s]\rPredicting:   9%|▉         | 1190/12660 [00:10<01:39, 115.64it/s]\rPredicting:   9%|▉         | 1202/12660 [00:10<01:38, 116.02it/s]\rPredicting:  10%|▉         | 1214/12660 [00:10<01:38, 115.84it/s]\rPredicting:  10%|▉         | 1226/12660 [00:11<01:39, 115.42it/s]\rPredicting:  10%|▉         | 1238/12660 [00:11<01:38, 115.81it/s]\rPredicting:  10%|▉         | 1250/12660 [00:11<01:38, 116.12it/s]\rPredicting:  10%|▉         | 1262/12660 [00:11<01:38, 116.26it/s]\rPredicting:  10%|█         | 1274/12660 [00:11<01:37, 116.30it/s]\rPredicting:  10%|█         | 1286/12660 [00:11<01:37, 116.78it/s]\rPredicting:  10%|█         | 1298/12660 [00:11<01:37, 116.49it/s]\rPredicting:  10%|█         | 1310/12660 [00:11<01:37, 116.22it/s]\rPredicting:  10%|█         | 1322/12660 [00:11<01:37, 116.24it/s]\rPredicting:  11%|█         | 1334/12660 [00:11<01:37, 116.63it/s]\rPredicting:  11%|█         | 1346/12660 [00:12<01:36, 116.92it/s]\rPredicting:  11%|█         | 1358/12660 [00:12<01:36, 116.97it/s]\rPredicting:  11%|█         | 1370/12660 [00:12<01:36, 117.19it/s]\rPredicting:  11%|█         | 1382/12660 [00:12<01:35, 117.53it/s]\rPredicting:  11%|█         | 1394/12660 [00:12<01:36, 117.33it/s]\rPredicting:  11%|█         | 1406/12660 [00:12<01:35, 117.27it/s]\rPredicting:  11%|█         | 1418/12660 [00:12<01:35, 117.17it/s]\rPredicting:  11%|█▏        | 1430/12660 [00:12<01:35, 117.42it/s]\rPredicting:  11%|█▏        | 1442/12660 [00:12<01:35, 117.65it/s]\rPredicting:  11%|█▏        | 1454/12660 [00:12<01:35, 117.62it/s]\rPredicting:  12%|█▏        | 1466/12660 [00:13<01:35, 117.52it/s]\rPredicting:  12%|█▏        | 1478/12660 [00:13<01:35, 117.29it/s]\rPredicting:  12%|█▏        | 1490/12660 [00:13<01:34, 117.71it/s]\rPredicting:  12%|█▏        | 1502/12660 [00:13<01:35, 117.38it/s]\rPredicting:  12%|█▏        | 1514/12660 [00:13<01:34, 117.44it/s]\rPredicting:  12%|█▏        | 1526/12660 [00:13<01:34, 117.22it/s]\rPredicting:  12%|█▏        | 1538/12660 [00:13<01:34, 117.37it/s]\rPredicting:  12%|█▏        | 1550/12660 [00:13<01:34, 117.61it/s]\rPredicting:  12%|█▏        | 1562/12660 [00:13<01:34, 117.36it/s]\rPredicting:  12%|█▏        | 1574/12660 [00:14<01:34, 117.32it/s]\rPredicting:  13%|█▎        | 1586/12660 [00:14<01:34, 116.88it/s]\rPredicting:  13%|█▎        | 1598/12660 [00:14<01:34, 117.21it/s]\rPredicting:  13%|█▎        | 1610/12660 [00:14<01:34, 117.21it/s]\rPredicting:  13%|█▎        | 1622/12660 [00:14<01:34, 117.05it/s]\rPredicting:  13%|█▎        | 1634/12660 [00:14<01:34, 116.95it/s]\rPredicting:  13%|█▎        | 1646/12660 [00:14<01:34, 116.87it/s]\rPredicting:  13%|█▎        | 1658/12660 [00:14<01:34, 116.98it/s]\rPredicting:  13%|█▎        | 1670/12660 [00:14<01:58, 92.68it/s] \rPredicting:  13%|█▎        | 1682/12660 [00:15<01:50, 99.18it/s]\rPredicting:  13%|█▎        | 1694/12660 [00:15<01:45, 104.25it/s]\rPredicting:  13%|█▎        | 1706/12660 [00:15<01:41, 107.83it/s]\rPredicting:  14%|█▎        | 1718/12660 [00:15<01:38, 110.62it/s]\rPredicting:  14%|█▎        | 1730/12660 [00:15<01:42, 107.11it/s]\rPredicting:  14%|█▍        | 1742/12660 [00:15<01:40, 108.17it/s]\rPredicting:  14%|█▍        | 1754/12660 [00:15<01:39, 109.43it/s]\rPredicting:  14%|█▍        | 1766/12660 [00:15<01:38, 110.26it/s]\rPredicting:  14%|█▍        | 1778/12660 [00:15<01:37, 111.86it/s]\rPredicting:  14%|█▍        | 1790/12660 [00:15<01:36, 112.47it/s]\rPredicting:  14%|█▍        | 1802/12660 [00:16<01:38, 110.68it/s]\rPredicting:  14%|█▍        | 1814/12660 [00:16<01:37, 111.78it/s]\rPredicting:  14%|█▍        | 1826/12660 [00:16<01:37, 111.48it/s]\rPredicting:  15%|█▍        | 1838/12660 [00:16<01:36, 112.48it/s]\rPredicting:  15%|█▍        | 1850/12660 [00:16<01:35, 112.72it/s]\rPredicting:  15%|█▍        | 1862/12660 [00:16<01:35, 112.64it/s]\rPredicting:  15%|█▍        | 1874/12660 [00:16<01:35, 113.17it/s]\rPredicting:  15%|█▍        | 1886/12660 [00:16<01:34, 113.67it/s]\rPredicting:  15%|█▍        | 1898/12660 [00:16<01:34, 113.31it/s]\rPredicting:  15%|█▌        | 1910/12660 [00:17<01:35, 112.88it/s]\rPredicting:  15%|█▌        | 1922/12660 [00:17<01:34, 113.09it/s]\rPredicting:  15%|█▌        | 1934/12660 [00:17<01:34, 113.03it/s]\rPredicting:  15%|█▌        | 1946/12660 [00:17<01:34, 113.26it/s]\rPredicting:  15%|█▌        | 1958/12660 [00:17<01:36, 111.14it/s]\rPredicting:  16%|█▌        | 1970/12660 [00:17<01:35, 112.13it/s]\rPredicting:  16%|█▌        | 1982/12660 [00:17<01:34, 112.92it/s]\rPredicting:  16%|█▌        | 1994/12660 [00:17<01:34, 112.98it/s]\rPredicting:  16%|█▌        | 2006/12660 [00:17<01:36, 110.66it/s]\rPredicting:  16%|█▌        | 2018/12660 [00:18<01:35, 110.89it/s]\rPredicting:  16%|█▌        | 2030/12660 [00:18<01:35, 111.43it/s]\rPredicting:  16%|█▌        | 2042/12660 [00:18<01:34, 112.00it/s]\rPredicting:  16%|█▌        | 2054/12660 [00:18<01:34, 112.66it/s]\rPredicting:  16%|█▋        | 2066/12660 [00:18<01:33, 113.31it/s]\rPredicting:  16%|█▋        | 2078/12660 [00:18<01:33, 112.95it/s]\rPredicting:  17%|█▋        | 2090/12660 [00:18<01:33, 112.90it/s]\rPredicting:  17%|█▋        | 2102/12660 [00:18<01:33, 113.26it/s]\rPredicting:  17%|█▋        | 2114/12660 [00:18<01:32, 113.59it/s]\rPredicting:  17%|█▋        | 2126/12660 [00:18<01:33, 112.54it/s]\rPredicting:  17%|█▋        | 2138/12660 [00:19<01:33, 112.01it/s]\rPredicting:  17%|█▋        | 2150/12660 [00:19<01:33, 112.20it/s]\rPredicting:  17%|█▋        | 2162/12660 [00:19<01:32, 113.61it/s]\rPredicting:  17%|█▋        | 2174/12660 [00:19<01:31, 114.69it/s]\rPredicting:  17%|█▋        | 2186/12660 [00:19<01:30, 115.37it/s]\rPredicting:  17%|█▋        | 2198/12660 [00:19<01:30, 115.32it/s]\rPredicting:  17%|█▋        | 2210/12660 [00:19<01:30, 115.04it/s]\rPredicting:  18%|█▊        | 2222/12660 [00:19<01:30, 115.19it/s]\rPredicting:  18%|█▊        | 2234/12660 [00:19<01:30, 115.03it/s]\rPredicting:  18%|█▊        | 2246/12660 [00:20<01:30, 115.68it/s]\rPredicting:  18%|█▊        | 2258/12660 [00:20<01:29, 116.14it/s]\rPredicting:  18%|█▊        | 2270/12660 [00:20<01:29, 116.02it/s]\rPredicting:  18%|█▊        | 2282/12660 [00:20<01:29, 116.54it/s]\rPredicting:  18%|█▊        | 2294/12660 [00:20<01:29, 116.38it/s]\rPredicting:  18%|█▊        | 2306/12660 [00:20<01:28, 116.50it/s]\rPredicting:  18%|█▊        | 2318/12660 [00:20<01:29, 115.95it/s]\rPredicting:  18%|█▊        | 2330/12660 [00:20<01:29, 115.70it/s]\rPredicting:  18%|█▊        | 2342/12660 [00:20<01:28, 116.19it/s]\rPredicting:  19%|█▊        | 2354/12660 [00:20<01:28, 116.07it/s]\rPredicting:  19%|█▊        | 2366/12660 [00:21<01:28, 115.75it/s]\rPredicting:  19%|█▉        | 2378/12660 [00:21<01:28, 115.68it/s]\rPredicting:  19%|█▉        | 2390/12660 [00:21<01:28, 115.76it/s]\rPredicting:  19%|█▉        | 2402/12660 [00:21<01:28, 115.49it/s]\rPredicting:  19%|█▉        | 2414/12660 [00:21<01:28, 116.08it/s]\rPredicting:  19%|█▉        | 2426/12660 [00:21<01:28, 116.29it/s]\rPredicting:  19%|█▉        | 2438/12660 [00:21<01:28, 115.70it/s]\rPredicting:  19%|█▉        | 2450/12660 [00:21<01:28, 115.83it/s]\rPredicting:  19%|█▉        | 2462/12660 [00:21<01:27, 116.06it/s]\rPredicting:  20%|█▉        | 2474/12660 [00:21<01:27, 115.89it/s]\rPredicting:  20%|█▉        | 2486/12660 [00:22<01:27, 116.06it/s]\rPredicting:  20%|█▉        | 2498/12660 [00:22<01:27, 115.98it/s]\rPredicting:  20%|█▉        | 2510/12660 [00:22<01:27, 115.67it/s]\rPredicting:  20%|█▉        | 2522/12660 [00:22<01:27, 115.85it/s]\rPredicting:  20%|██        | 2534/12660 [00:22<01:27, 115.52it/s]\rPredicting:  20%|██        | 2546/12660 [00:22<01:27, 115.48it/s]\rPredicting:  20%|██        | 2558/12660 [00:22<01:27, 115.17it/s]\rPredicting:  20%|██        | 2570/12660 [00:22<01:27, 115.32it/s]\rPredicting:  20%|██        | 2582/12660 [00:22<01:27, 115.00it/s]\rPredicting:  20%|██        | 2594/12660 [00:23<01:27, 115.20it/s]\rPredicting:  21%|██        | 2606/12660 [00:23<02:11, 76.30it/s] \rPredicting:  21%|██        | 2618/12660 [00:23<01:58, 84.83it/s]\rPredicting:  21%|██        | 2630/12660 [00:23<01:48, 92.10it/s]\rPredicting:  21%|██        | 2642/12660 [00:23<01:42, 98.18it/s]\rPredicting:  21%|██        | 2654/12660 [00:23<01:37, 102.84it/s]\rPredicting:  21%|██        | 2666/12660 [00:23<01:34, 106.14it/s]\rPredicting:  21%|██        | 2678/12660 [00:23<01:32, 107.98it/s]\rPredicting:  21%|██        | 2690/12660 [00:24<01:30, 110.33it/s]\rPredicting:  21%|██▏       | 2702/12660 [00:24<01:29, 111.76it/s]\rPredicting:  21%|██▏       | 2714/12660 [00:24<01:28, 112.02it/s]\rPredicting:  22%|██▏       | 2726/12660 [00:24<01:28, 112.85it/s]\rPredicting:  22%|██▏       | 2738/12660 [00:24<01:27, 113.58it/s]\rPredicting:  22%|██▏       | 2750/12660 [00:24<01:26, 114.18it/s]\rPredicting:  22%|██▏       | 2762/12660 [00:24<01:26, 114.51it/s]\rPredicting:  22%|██▏       | 2774/12660 [00:24<01:26, 114.74it/s]\rPredicting:  22%|██▏       | 2786/12660 [00:24<01:26, 114.74it/s]\rPredicting:  22%|██▏       | 2798/12660 [00:24<01:25, 114.81it/s]\rPredicting:  22%|██▏       | 2810/12660 [00:25<01:25, 115.25it/s]\rPredicting:  22%|██▏       | 2822/12660 [00:25<01:25, 114.94it/s]\rPredicting:  22%|██▏       | 2834/12660 [00:25<01:25, 114.71it/s]\rPredicting:  22%|██▏       | 2846/12660 [00:25<01:25, 114.94it/s]\rPredicting:  23%|██▎       | 2858/12660 [00:25<01:24, 115.57it/s]\rPredicting:  23%|██▎       | 2870/12660 [00:25<01:24, 115.87it/s]\rPredicting:  23%|██▎       | 2882/12660 [00:25<01:24, 115.99it/s]\rPredicting:  23%|██▎       | 2894/12660 [00:25<01:24, 116.05it/s]\rPredicting:  23%|██▎       | 2906/12660 [00:25<01:23, 116.20it/s]\rPredicting:  23%|██▎       | 2918/12660 [00:26<01:23, 116.23it/s]\rPredicting:  23%|██▎       | 2930/12660 [00:26<01:23, 116.02it/s]\rPredicting:  23%|██▎       | 2942/12660 [00:26<01:23, 116.13it/s]\rPredicting:  23%|██▎       | 2954/12660 [00:26<01:23, 116.04it/s]\rPredicting:  23%|██▎       | 2966/12660 [00:26<01:23, 116.08it/s]\rPredicting:  24%|██▎       | 2978/12660 [00:26<01:23, 116.31it/s]\rPredicting:  24%|██▎       | 2990/12660 [00:26<01:23, 116.48it/s]\rPredicting:  24%|██▎       | 3002/12660 [00:26<01:22, 116.42it/s]\rPredicting:  24%|██▍       | 3014/12660 [00:26<01:22, 116.66it/s]\rPredicting:  24%|██▍       | 3026/12660 [00:26<01:22, 116.25it/s]\rPredicting:  24%|██▍       | 3038/12660 [00:27<01:22, 116.14it/s]\rPredicting:  24%|██▍       | 3050/12660 [00:27<01:23, 115.59it/s]\rPredicting:  24%|██▍       | 3062/12660 [00:27<01:24, 113.58it/s]\rPredicting:  24%|██▍       | 3074/12660 [00:27<01:24, 113.55it/s]\rPredicting:  24%|██▍       | 3086/12660 [00:27<01:24, 113.82it/s]\rPredicting:  24%|██▍       | 3098/12660 [00:27<01:23, 114.20it/s]\rPredicting:  25%|██▍       | 3110/12660 [00:27<01:23, 114.89it/s]\rPredicting:  25%|██▍       | 3122/12660 [00:27<01:22, 115.02it/s]\rPredicting:  25%|██▍       | 3134/12660 [00:27<01:22, 115.29it/s]\rPredicting:  25%|██▍       | 3146/12660 [00:27<01:22, 115.59it/s]\rPredicting:  25%|██▍       | 3158/12660 [00:28<01:22, 115.59it/s]\rPredicting:  25%|██▌       | 3170/12660 [00:28<01:22, 115.30it/s]\rPredicting:  25%|██▌       | 3182/12660 [00:28<01:22, 115.34it/s]\rPredicting:  25%|██▌       | 3194/12660 [00:28<01:22, 115.29it/s]\rPredicting:  25%|██▌       | 3206/12660 [00:28<01:22, 115.23it/s]\rPredicting:  25%|██▌       | 3218/12660 [00:28<01:22, 114.99it/s]\rPredicting:  26%|██▌       | 3230/12660 [00:28<01:21, 115.42it/s]\rPredicting:  26%|██▌       | 3242/12660 [00:28<01:21, 115.44it/s]\rPredicting:  26%|██▌       | 3254/12660 [00:28<01:21, 115.57it/s]\rPredicting:  26%|██▌       | 3266/12660 [00:29<01:21, 115.57it/s]\rPredicting:  26%|██▌       | 3278/12660 [00:29<01:21, 115.31it/s]\rPredicting:  26%|██▌       | 3290/12660 [00:29<01:21, 115.50it/s]\rPredicting:  26%|██▌       | 3302/12660 [00:29<01:20, 115.74it/s]\rPredicting:  26%|██▌       | 3314/12660 [00:29<01:20, 115.68it/s]\rPredicting:  26%|██▋       | 3326/12660 [00:29<01:20, 116.04it/s]\rPredicting:  26%|██▋       | 3338/12660 [00:29<01:20, 116.34it/s]\rPredicting:  26%|██▋       | 3350/12660 [00:29<01:20, 116.30it/s]\rPredicting:  27%|██▋       | 3362/12660 [00:29<01:19, 116.25it/s]\rPredicting:  27%|██▋       | 3374/12660 [00:29<01:19, 116.38it/s]\rPredicting:  27%|██▋       | 3386/12660 [00:30<01:19, 116.32it/s]\rPredicting:  27%|██▋       | 3398/12660 [00:30<01:19, 116.13it/s]\rPredicting:  27%|██▋       | 3410/12660 [00:30<01:19, 115.94it/s]\rPredicting:  27%|██▋       | 3422/12660 [00:30<01:19, 116.01it/s]\rPredicting:  27%|██▋       | 3434/12660 [00:30<01:19, 116.20it/s]\rPredicting:  27%|██▋       | 3446/12660 [00:30<01:18, 116.63it/s]\rPredicting:  27%|██▋       | 3458/12660 [00:30<01:18, 116.52it/s]\rPredicting:  27%|██▋       | 3470/12660 [00:30<01:18, 116.60it/s]\rPredicting:  28%|██▊       | 3482/12660 [00:30<01:18, 116.58it/s]\rPredicting:  28%|██▊       | 3494/12660 [00:30<01:18, 116.26it/s]\rPredicting:  28%|██▊       | 3506/12660 [00:31<01:19, 115.61it/s]\rPredicting:  28%|██▊       | 3518/12660 [00:31<01:18, 115.79it/s]\rPredicting:  28%|██▊       | 3530/12660 [00:31<01:18, 116.10it/s]\rPredicting:  28%|██▊       | 3542/12660 [00:31<01:18, 115.69it/s]\rPredicting:  28%|██▊       | 3554/12660 [00:31<01:18, 115.71it/s]\rPredicting:  28%|██▊       | 3566/12660 [00:31<01:18, 116.16it/s]\rPredicting:  28%|██▊       | 3578/12660 [00:31<01:17, 116.46it/s]\rPredicting:  28%|██▊       | 3590/12660 [00:31<01:18, 115.30it/s]\rPredicting:  28%|██▊       | 3602/12660 [00:31<01:18, 116.01it/s]\rPredicting:  29%|██▊       | 3614/12660 [00:32<01:18, 115.35it/s]\rPredicting:  29%|██▊       | 3626/12660 [00:32<01:18, 115.65it/s]\rPredicting:  29%|██▊       | 3638/12660 [00:32<01:18, 115.38it/s]\rPredicting:  29%|██▉       | 3650/12660 [00:32<01:18, 115.47it/s]\rPredicting:  29%|██▉       | 3662/12660 [00:32<01:17, 115.70it/s]\rPredicting:  29%|██▉       | 3674/12660 [00:32<01:17, 115.93it/s]\rPredicting:  29%|██▉       | 3686/12660 [00:32<01:17, 115.13it/s]\rPredicting:  29%|██▉       | 3698/12660 [00:32<01:17, 115.71it/s]\rPredicting:  29%|██▉       | 3710/12660 [00:32<01:17, 115.80it/s]\rPredicting:  29%|██▉       | 3722/12660 [00:32<01:17, 115.75it/s]\rPredicting:  29%|██▉       | 3734/12660 [00:33<01:17, 115.82it/s]\rPredicting:  30%|██▉       | 3746/12660 [00:33<01:16, 115.99it/s]\rPredicting:  30%|██▉       | 3758/12660 [00:33<01:16, 115.89it/s]\rPredicting:  30%|██▉       | 3770/12660 [00:33<01:17, 115.32it/s]\rPredicting:  30%|██▉       | 3782/12660 [00:33<01:16, 115.78it/s]\rPredicting:  30%|██▉       | 3794/12660 [00:33<01:16, 115.90it/s]\rPredicting:  30%|███       | 3806/12660 [00:33<01:16, 115.67it/s]\rPredicting:  30%|███       | 3818/12660 [00:33<01:16, 115.91it/s]\rPredicting:  30%|███       | 3830/12660 [00:33<01:16, 115.74it/s]\rPredicting:  30%|███       | 3842/12660 [00:33<01:16, 115.58it/s]\rPredicting:  30%|███       | 3854/12660 [00:34<01:16, 115.66it/s]\rPredicting:  31%|███       | 3866/12660 [00:34<01:16, 115.33it/s]\rPredicting:  31%|███       | 3878/12660 [00:34<01:16, 115.55it/s]\rPredicting:  31%|███       | 3890/12660 [00:34<01:15, 115.62it/s]\rPredicting:  31%|███       | 3902/12660 [00:34<01:15, 115.29it/s]\rPredicting:  31%|███       | 3914/12660 [00:34<01:15, 115.32it/s]\rPredicting:  31%|███       | 3926/12660 [00:34<01:15, 115.44it/s]\rPredicting:  31%|███       | 3938/12660 [00:34<01:15, 115.58it/s]\rPredicting:  31%|███       | 3950/12660 [00:34<01:15, 115.50it/s]\rPredicting:  31%|███▏      | 3962/12660 [00:35<01:15, 115.76it/s]\rPredicting:  31%|███▏      | 3974/12660 [00:35<01:14, 115.88it/s]\rPredicting:  31%|███▏      | 3986/12660 [00:35<01:15, 115.17it/s]\rPredicting:  32%|███▏      | 3998/12660 [00:35<01:15, 115.23it/s]\rPredicting:  32%|███▏      | 4010/12660 [00:35<01:15, 115.26it/s]\rPredicting:  32%|███▏      | 4022/12660 [00:35<01:15, 115.15it/s]\rPredicting:  32%|███▏      | 4034/12660 [00:35<01:14, 115.41it/s]\rPredicting:  32%|███▏      | 4046/12660 [00:35<01:14, 114.98it/s]\rPredicting:  32%|███▏      | 4058/12660 [00:35<01:14, 114.89it/s]\rPredicting:  32%|███▏      | 4070/12660 [00:35<01:14, 114.81it/s]\rPredicting:  32%|███▏      | 4082/12660 [00:36<01:14, 114.46it/s]\rPredicting:  32%|███▏      | 4094/12660 [00:36<01:14, 114.91it/s]\rPredicting:  32%|███▏      | 4106/12660 [00:36<01:14, 115.32it/s]\rPredicting:  33%|███▎      | 4118/12660 [00:36<01:14, 115.29it/s]\rPredicting:  33%|███▎      | 4130/12660 [00:36<01:14, 115.02it/s]\rPredicting:  33%|███▎      | 4142/12660 [00:36<01:13, 115.18it/s]\rPredicting:  33%|███▎      | 4154/12660 [00:36<01:14, 114.70it/s]\rPredicting:  33%|███▎      | 4166/12660 [00:36<01:14, 114.66it/s]\rPredicting:  33%|███▎      | 4178/12660 [00:36<01:14, 114.17it/s]\rPredicting:  33%|███▎      | 4190/12660 [00:37<01:14, 114.28it/s]\rPredicting:  33%|███▎      | 4202/12660 [00:37<01:13, 114.53it/s]\rPredicting:  33%|███▎      | 4214/12660 [00:37<01:14, 114.08it/s]\rPredicting:  33%|███▎      | 4226/12660 [00:37<01:13, 114.39it/s]\rPredicting:  33%|███▎      | 4238/12660 [00:37<01:13, 114.54it/s]\rPredicting:  34%|███▎      | 4250/12660 [00:37<01:13, 114.65it/s]\rPredicting:  34%|███▎      | 4262/12660 [00:37<01:13, 114.62it/s]\rPredicting:  34%|███▍      | 4274/12660 [00:37<01:12, 114.92it/s]\rPredicting:  34%|███▍      | 4286/12660 [00:37<01:12, 114.77it/s]\rPredicting:  34%|███▍      | 4298/12660 [00:37<01:12, 114.66it/s]\rPredicting:  34%|███▍      | 4310/12660 [00:38<01:12, 114.67it/s]\rPredicting:  34%|███▍      | 4322/12660 [00:38<01:12, 115.07it/s]\rPredicting:  34%|███▍      | 4334/12660 [00:38<01:12, 114.80\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n0 [01:11<00:39, 114.34it/s]\rPredicting:  65%|██████▍   | 8174/12660 [01:11<00:39, 114.31it/s]\rPredicting:  65%|██████▍   | 8186/12660 [01:11<00:39, 114.34it/s]\rPredicting:  65%|██████▍   | 8198/12660 [01:11<00:38, 114.53it/s]\rPredicting:  65%|██████▍   | 8210/12660 [01:11<00:38, 114.97it/s]\rPredicting:  65%|██████▍   | 8222/12660 [01:12<00:38, 115.09it/s]\rPredicting:  65%|██████▌   | 8234/12660 [01:12<00:38, 114.96it/s]\rPredicting:  65%|██████▌   | 8246/12660 [01:12<00:38, 114.94it/s]\rPredicting:  65%|██████▌   | 8258/12660 [01:12<00:38, 115.10it/s]\rPredicting:  65%|██████▌   | 8270/12660 [01:12<00:38, 114.82it/s]\rPredicting:  65%|██████▌   | 8282/12660 [01:12<00:38, 114.88it/s]\rPredicting:  66%|██████▌   | 8294/12660 [01:12<00:37, 115.32it/s]\rPredicting:  66%|██████▌   | 8306/12660 [01:12<00:37, 115.54it/s]\rPredicting:  66%|██████▌   | 8318/12660 [01:12<00:37, 115.42it/s]\rPredicting:  66%|██████▌   | 8330/12660 [01:12<00:37, 115.26it/s]\rPredicting:  66%|██████▌   | 8342/12660 [01:13<00:37, 115.32it/s]\rPredicting:  66%|██████▌   | 8354/12660 [01:13<00:37, 115.52it/s]\rPredicting:  66%|██████▌   | 8366/12660 [01:13<00:37, 115.37it/s]\rPredicting:  66%|██████▌   | 8378/12660 [01:13<00:37, 115.17it/s]\rPredicting:  66%|██████▋   | 8390/12660 [01:13<00:37, 115.16it/s]\rPredicting:  66%|██████▋   | 8402/12660 [01:13<00:36, 115.17it/s]\rPredicting:  66%|██████▋   | 8414/12660 [01:13<00:36, 115.04it/s]\rPredicting:  67%|██████▋   | 8426/12660 [01:13<00:36, 114.94it/s]\rPredicting:  67%|██████▋   | 8438/12660 [01:13<00:36, 114.99it/s]\rPredicting:  67%|██████▋   | 8450/12660 [01:14<00:36, 115.22it/s]\rPredicting:  67%|██████▋   | 8462/12660 [01:14<00:36, 113.65it/s]\rPredicting:  67%|██████▋   | 8474/12660 [01:14<00:36, 114.40it/s]\rPredicting:  67%|██████▋   | 8486/12660 [01:14<00:36, 114.46it/s]\rPredicting:  67%|██████▋   | 8498/12660 [01:14<00:36, 115.16it/s]\rPredicting:  67%|██████▋   | 8510/12660 [01:14<00:35, 115.49it/s]\rPredicting:  67%|██████▋   | 8522/12660 [01:14<00:35, 115.39it/s]\rPredicting:  67%|██████▋   | 8534/12660 [01:14<00:35, 115.41it/s]\rPredicting:  68%|██████▊   | 8546/12660 [01:14<00:35, 115.36it/s]\rPredicting:  68%|██████▊   | 8558/12660 [01:14<00:35, 115.23it/s]\rPredicting:  68%|██████▊   | 8570/12660 [01:15<00:35, 114.89it/s]\rPredicting:  68%|██████▊   | 8582/12660 [01:15<00:35, 114.85it/s]\rPredicting:  68%|██████▊   | 8594/12660 [01:15<00:35, 114.64it/s]\rPredicting:  68%|██████▊   | 8606/12660 [01:15<00:35, 114.63it/s]\rPredicting:  68%|██████▊   | 8618/12660 [01:15<00:35, 114.47it/s]\rPredicting:  68%|██████▊   | 8630/12660 [01:15<00:35, 114.60it/s]\rPredicting:  68%|██████▊   | 8642/12660 [01:15<00:34, 114.99it/s]\rPredicting:  68%|██████▊   | 8654/12660 [01:15<00:34, 114.92it/s]\rPredicting:  68%|██████▊   | 8666/12660 [01:15<00:34, 114.52it/s]\rPredicting:  69%|██████▊   | 8678/12660 [01:15<00:34, 114.65it/s]\rPredicting:  69%|██████▊   | 8690/12660 [01:16<00:34, 114.79it/s]\rPredicting:  69%|██████▊   | 8702/12660 [01:16<00:34, 114.86it/s]\rPredicting:  69%|██████▉   | 8714/12660 [01:16<00:34, 114.70it/s]\rPredicting:  69%|██████▉   | 8726/12660 [01:16<00:34, 114.81it/s]\rPredicting:  69%|██████▉   | 8738/12660 [01:16<00:34, 114.94it/s]\rPredicting:  69%|██████▉   | 8750/12660 [01:16<00:33, 115.00it/s]\rPredicting:  69%|██████▉   | 8762/12660 [01:16<00:33, 114.81it/s]\rPredicting:  69%|██████▉   | 8774/12660 [01:16<00:33, 114.79it/s]\rPredicting:  69%|██████▉   | 8786/12660 [01:16<00:33, 114.82it/s]\rPredicting:  69%|██████▉   | 8798/12660 [01:17<00:33, 114.98it/s]\rPredicting:  70%|██████▉   | 8810/12660 [01:17<00:33, 114.90it/s]\rPredicting:  70%|██████▉   | 8822/12660 [01:17<00:33, 113.78it/s]\rPredicting:  70%|██████▉   | 8834/12660 [01:17<00:33, 114.32it/s]\rPredicting:  70%|██████▉   | 8846/12660 [01:17<00:33, 114.31it/s]\rPredicting:  70%|██████▉   | 8858/12660 [01:17<00:33, 114.73it/s]\rPredicting:  70%|███████   | 8870/12660 [01:17<00:33, 114.38it/s]\rPredicting:  70%|███████   | 8882/12660 [01:17<00:33, 114.31it/s]\rPredicting:  70%|███████   | 8894/12660 [01:17<00:32, 114.49it/s]\rPredicting:  70%|███████   | 8906/12660 [01:17<00:32, 114.73it/s]\rPredicting:  70%|███████   | 8918/12660 [01:18<00:32, 115.15it/s]\rPredicting:  71%|███████   | 8930/12660 [01:18<00:32, 115.23it/s]\rPredicting:  71%|███████   | 8942/12660 [01:18<00:32, 114.97it/s]\rPredicting:  71%|███████   | 8954/12660 [01:18<00:32, 114.95it/s]\rPredicting:  71%|███████   | 8966/12660 [01:18<00:32, 115.02it/s]\rPredicting:  71%|███████   | 8978/12660 [01:18<00:32, 114.77it/s]\rPredicting:  71%|███████   | 8990/12660 [01:18<00:31, 114.97it/s]\rPredicting:  71%|███████   | 9002/12660 [01:18<00:31, 115.08it/s]\rPredicting:  71%|███████   | 9014/12660 [01:18<00:31, 115.41it/s]\rPredicting:  71%|███████▏  | 9026/12660 [01:19<00:31, 115.36it/s]\rPredicting:  71%|███████▏  | 9038/12660 [01:19<00:31, 114.88it/s]\rPredicting:  71%|███████▏  | 9050/12660 [01:19<00:31, 115.21it/s]\rPredicting:  72%|███████▏  | 9062/12660 [01:19<00:31, 114.96it/s]\rPredicting:  72%|███████▏  | 9074/12660 [01:19<00:31, 115.28it/s]\rPredicting:  72%|███████▏  | 9086/12660 [01:19<00:31, 115.18it/s]\rPredicting:  72%|███████▏  | 9098/12660 [01:19<00:30, 115.07it/s]\rPredicting:  72%|███████▏  | 9110/12660 [01:19<00:30, 115.19it/s]\rPredicting:  72%|███████▏  | 9122/12660 [01:19<00:30, 115.47it/s]\rPredicting:  72%|███████▏  | 9134/12660 [01:19<00:30, 115.52it/s]\rPredicting:  72%|███████▏  | 9146/12660 [01:20<00:30, 115.39it/s]\rPredicting:  72%|███████▏  | 9158/12660 [01:20<00:30, 115.63it/s]\rPredicting:  72%|███████▏  | 9170/12660 [01:20<00:30, 115.47it/s]\rPredicting:  73%|███████▎  | 9182/12660 [01:20<00:30, 115.78it/s]\rPredicting:  73%|███████▎  | 9194/12660 [01:20<00:30, 115.40it/s]\rPredicting:  73%|███████▎  | 9206/12660 [01:20<00:29, 115.20it/s]\rPredicting:  73%|███████▎  | 9218/12660 [01:20<00:29, 115.14it/s]\rPredicting:  73%|███████▎  | 9230/12660 [01:20<00:29, 115.26it/s]\rPredicting:  73%|███████▎  | 9242/12660 [01:20<00:29, 115.39it/s]\rPredicting:  73%|███████▎  | 9254/12660 [01:20<00:29, 114.96it/s]\rPredicting:  73%|███████▎  | 9266/12660 [01:21<00:29, 114.64it/s]\rPredicting:  73%|███████▎  | 9278/12660 [01:21<00:29, 114.91it/s]\rPredicting:  73%|███████▎  | 9290/12660 [01:21<00:29, 115.50it/s]\rPredicting:  73%|███████▎  | 9302/12660 [01:21<00:28, 115.82it/s]\rPredicting:  74%|███████▎  | 9314/12660 [01:21<00:28, 115.78it/s]\rPredicting:  74%|███████▎  | 9326/12660 [01:21<00:28, 115.83it/s]\rPredicting:  74%|███████▍  | 9338/12660 [01:21<00:28, 115.53it/s]\rPredicting:  74%|███████▍  | 9350/12660 [01:21<00:28, 115.62it/s]\rPredicting:  74%|███████▍  | 9362/12660 [01:21<00:28, 115.59it/s]\rPredicting:  74%|███████▍  | 9374/12660 [01:22<00:28, 115.35it/s]\rPredicting:  74%|███████▍  | 9386/12660 [01:22<00:28, 115.05it/s]\rPredicting:  74%|███████▍  | 9398/12660 [01:22<00:28, 114.80it/s]\rPredicting:  74%|███████▍  | 9410/12660 [01:22<00:28, 115.02it/s]\rPredicting:  74%|███████▍  | 9422/12660 [01:22<00:28, 115.10it/s]\rPredicting:  75%|███████▍  | 9434/12660 [01:22<00:28, 115.10it/s]\rPredicting:  75%|███████▍  | 9446/12660 [01:22<00:27, 115.16it/s]\rPredicting:  75%|███████▍  | 9458/12660 [01:22<00:27, 114.97it/s]\rPredicting:  75%|███████▍  | 9470/12660 [01:22<00:27, 115.10it/s]\rPredicting:  75%|███████▍  | 9482/12660 [01:22<00:27, 115.15it/s]\rPredicting:  75%|███████▍  | 9494/12660 [01:23<00:27, 115.51it/s]\rPredicting:  75%|███████▌  | 9506/12660 [01:23<00:27, 115.25it/s]\rPredicting:  75%|███████▌  | 9518/12660 [01:23<00:27, 115.03it/s]\rPredicting:  75%|███████▌  | 9530/12660 [01:23<00:27, 114.93it/s]\rPredicting:  75%|███████▌  | 9542/12660 [01:23<00:27, 115.44it/s]\rPredicting:  75%|███████▌  | 9554/12660 [01:23<00:26, 115.61it/s]\rPredicting:  76%|███████▌  | 9566/12660 [01:23<00:26, 115.52it/s]\rPredicting:  76%|███████▌  | 9578/12660 [01:23<00:26, 115.63it/s]\rPredicting:  76%|███████▌  | 9590/12660 [01:23<00:26, 115.68it/s]\rPredicting:  76%|███████▌  | 9602/12660 [01:24<00:26, 115.36it/s]\rPredicting:  76%|███████▌  | 9614/12660 [01:24<00:26, 115.01it/s]\rPredicting:  76%|███████▌  | 9626/12660 [01:24<00:26, 114.99it/s]\rPredicting:  76%|███████▌  | 9638/12660 [01:24<00:26, 114.95it/s]\rPredicting:  76%|███████▌  | 9650/12660 [01:24<00:26, 114.88it/s]\rPredicting:  76%|███████▋  | 9662/12660 [01:24<00:26, 115.21it/s]\rPredicting:  76%|███████▋  | 9674/12660 [01:24<00:25, 115.45it/s]\rPredicting:  77%|███████▋  | 9686/12660 [01:24<00:25, 115.37it/s]\rPredicting:  77%|███████▋  | 9698/12660 [01:24<00:25, 115.58it/s]\rPredicting:  77%|███████▋  | 9710/12660 [01:24<00:25, 115.72it/s]\rPredicting:  77%|███████▋  | 9722/12660 [01:25<00:25, 115.73it/s]\rPredicting:  77%|███████▋  | 9734/12660 [01:25<00:25, 116.06it/s]\rPredicting:  77%|███████▋  | 9746/12660 [01:25<00:25, 115.55it/s]\rPredicting:  77%|███████▋  | 9758/12660 [01:25<00:25, 116.02it/s]\rPredicting:  77%|███████▋  | 9770/12660 [01:25<00:24, 115.69it/s]\rPredicting:  77%|███████▋  | 9782/12660 [01:25<00:25, 115.08it/s]\rPredicting:  77%|███████▋  | 9794/12660 [01:25<00:24, 115.11it/s]\rPredicting:  77%|███████▋  | 9806/12660 [01:25<00:24, 114.80it/s]\rPredicting:  78%|███████▊  | 9818/12660 [01:25<00:24, 114.59it/s]\rPredicting:  78%|███████▊  | 9830/12660 [01:25<00:24, 114.53it/s]\rPredicting:  78%|███████▊  | 9842/12660 [01:26<00:24, 114.77it/s]\rPredicting:  78%|███████▊  | 9854/12660 [01:26<00:24, 114.60it/s]\rPredicting:  78%|███████▊  | 9866/12660 [01:26<00:24, 114.62it/s]\rPredicting:  78%|███████▊  | 9878/12660 [01:26<00:24, 114.20it/s]\rPredicting:  78%|███████▊  | 9890/12660 [01:26<00:24, 114.54it/s]\rPredicting:  78%|███████▊  | 9902/12660 [01:26<00:24, 114.62it/s]\rPredicting:  78%|███████▊  | 9914/12660 [01:26<00:23, 114.62it/s]\rPredicting:  78%|███████▊  | 9926/12660 [01:26<00:23, 114.51it/s]\rPredicting:  78%|███████▊  | 9938/12660 [01:26<00:23, 114.76it/s]\rPredicting:  79%|███████▊  | 9950/12660 [01:27<00:23, 114.72it/s]\rPredicting:  79%|███████▊  | 9962/12660 [01:27<00:24, 112.40it/s]\rPredicting:  79%|███████▉  | 9974/12660 [01:27<00:23, 112.49it/s]\rPredicting:  79%|███████▉  | 9986/12660 [01:27<00:23, 113.55it/s]\rPredicting:  79%|███████▉  | 9998/12660 [01:27<00:23, 114.09it/s]\rPredicting:  79%|███████▉  | 10010/12660 [01:27<00:23, 114.41it/s]\rPredicting:  79%|███████▉  | 10022/12660 [01:27<00:23, 114.40it/s]\rPredicting:  79%|███████▉  | 10034/12660 [01:27<00:22, 114.41it/s]\rPredicting:  79%|███████▉  | 10046/12660 [01:27<00:22, 114.25it/s]\rPredicting:  79%|███████▉  | 10058/12660 [01:27<00:22, 114.54it/s]\rPredicting:  80%|███████▉  | 10070/12660 [01:28<00:22, 114.60it/s]\rPredicting:  80%|███████▉  | 10082/12660 [01:28<00:22, 115.07it/s]\rPredicting:  80%|███████▉  | 10094/12660 [01:28<00:22, 115.19it/s]\rPredicting:  80%|███████▉  | 10106/12660 [01:28<00:22, 115.63it/s]\rPredicting:  80%|███████▉  | 10118/12660 [01:28<00:21, 115.61it/s]\rPredicting:  80%|████████  | 10130/12660 [01:28<00:21, 115.77it/s]\rPredicting:  80%|████████  | 10142/12660 [01:28<00:21, 115.58it/s]\rPredicting:  80%|████████  | 10154/12660 [01:28<00:21, 115.21it/s]\rPredicting:  80%|████████  | 10166/12660 [01:28<00:21, 114.91it/s]\rPredicting:  80%|████████  | 10178/12660 [01:29<00:21, 115.15it/s]\rPredicting:  80%|████████  | 10190/12660 [01:29<00:21, 115.34it/s]\rPredicting:  81%|████████  | 10202/12660 [01:29<00:21, 115.60it/s]\rPredicting:  81%|████████  | 10214/12660 [01:29<00:21, 115.73it/s]\rPredicting:  81%|████████  | 10226/12660 [01:29<00:21, 115.29it/s]\rPredicting:  81%|████████  | 10238/12660 [01:29<00:21, 115.30it/s]\rPredicting:  81%|████████  | 10250/12660 [01:29<00:20, 115.41it/s]\rPredicting:  81%|████████  | 10262/12660 [01:29<00:20, 115.60it/s]\rPredicting:  81%|████████  | 10274/12660 [01:29<00:20, 115.46it/s]\rPredicting:  81%|████████  | 10286/12660 [01:29<00:20, 115.41it/s]\rPredicting:  81%|████████▏ | 10298/12660 [01:30<00:20, 115.12it/s]\rPredicting:  81%|████████▏ | 10310/12660 [01:30<00:20, 114.90it/s]\rPredicting:  82%|████████▏ | 10322/12660 [01:30<00:20, 114.71it/s]\rPredicting:  82%|████████▏ | 10334/12660 [01:30<00:20, 114.84it/s]\rPredicting:  82%|████████▏ | 10346/12660 [01:30<00:20, 115.32it/s]\rPredicting:  82%|████████▏ | 10358/12660 [01:30<00:19, 115.35it/s]\rPredicting:  82%|████████▏ | 10370/12660 [01:30<00:19, 114.96it/s]\rPredicting:  82%|████████▏ | 10382/12660 [01:30<00:19, 114.67it/s]\rPredicting:  82%|████████▏ | 10394/12660 [01:30<00:19, 114.28it/s]\rPredicting:  82%|████████▏ | 10406/12660 [01:31<00:19, 113.22it/s]\rPredicting:  82%|████████▏ | 10418/12660 [01:31<00:19, 114.01it/s]\rPredicting:  82%|████████▏ | 10430/12660 [01:31<00:19, 114.67it/s]\rPredicting:  82%|████████▏ | 10442/12660 [01:31<00:19, 114.71it/s]\rPredicting:  83%|████████▎ | 10454/12660 [01:31<00:19, 114.90it/s]\rPredicting:  83%|████████▎ | 10466/12660 [01:31<00:19, 114.82it/s]\rPredicting:  83%|████████▎ | 10478/12660 [01:31<00:18, 114.98it/s]\rPredicting:  83%|████████▎ | 10490/12660 [01:31<00:18, 115.15it/s]\rPredicting:  83%|████████▎ | 10502/12660 [01:31<00:18, 115.10it/s]\rPredicting:  83%|████████▎ | 10514/12660 [01:31<00:18, 114.99it/s]\rPredicting:  83%|████████▎ | 10526/12660 [01:32<00:18, 115.14it/s]\rPredicting:  83%|████████▎ | 10538/12660 [01:32<00:18, 115.04it/s]\rPredicting:  83%|████████▎ | 10550/12660 [01:32<00:18, 115.25it/s]\rPredicting:  83%|████████▎ | 10562/12660 [01:32<00:18, 115.49it/s]\rPredicting:  84%|████████▎ | 10574/12660 [01:32<00:18, 115.22it/s]\rPredicting:  84%|████████▎ | 10586/12660 [01:32<00:17, 115.30it/s]\rPredicting:  84%|████████▎ | 10598/12660 [01:32<00:18, 114.42it/s]\rPredicting:  84%|████████▍ | 10610/12660 [01:32<00:17, 114.82it/s]\rPredicting:  84%|████████▍ | 10622/12660 [01:32<00:17, 114.50it/s]\rPredicting:  84%|████████▍ | 10634/12660 [01:32<00:17, 114.33it/s]\rPredicting:  84%|████████▍ | 10646/12660 [01:33<00:17, 114.47it/s]\rPredicting:  84%|████████▍ | 10658/12660 [01:33<00:17, 114.60it/s]\rPredicting:  84%|████████▍ | 10670/12660 [01:33<00:17, 114.39it/s]\rPredicting:  84%|████████▍ | 10682/12660 [01:33<00:17, 114.30it/s]\rPredicting:  84%|████████▍ | 10694/12660 [01:33<00:17, 114.59it/s]\rPredicting:  85%|████████▍ | 10706/12660 [01:33<00:17, 114.19it/s]\rPredicting:  85%|████████▍ | 10718/12660 [01:33<00:17, 114.22it/s]\rPredicting:  85%|████████▍ | 10730/12660 [01:33<00:16, 114.32it/s]\rPredicting:  85%|████████▍ | 10742/12660 [01:33<00:16, 114.45it/s]\rPredicting:  85%|████████▍ | 10754/12660 [01:34<00:16, 114.74it/s]\rPredicting:  85%|████████▌ | 10766/12660 [01:34<00:16, 114.76it/s]\rPredicting:  85%|████████▌ | 10778/12660 [01:34<00:16, 114.98it/s]\rPredicting:  85%|████████▌ | 10790/12660 [01:34<00:16, 115.23it/s]\rPredicting:  85%|████████▌ | 10802/12660 [01:34<00:16, 114.81it/s]\rPredicting:  85%|████████▌ | 10814/12660 [01:34<00:16, 114.79it/s]\rPredicting:  86%|████████▌ | 10826/12660 [01:34<00:15, 115.13it/s]\rPredicting:  86%|████████▌ | 10838/12660 [01:34<00:15, 115.00it/s]\rPredicting:  86%|████████▌ | 10850/12660 [01:34<00:15, 115.37it/s]\rPredicting:  86%|████████▌ | 10862/12660 [01:34<00:15, 115.29it/s]\rPredicting:  86%|████████▌ | 10874/12660 [01:35<00:15, 115.52it/s]\rPredicting:  86%|████████▌ | 10886/12660 [01:35<00:15, 115.70it/s]\rPredicting:  86%|████████▌ | 10898/12660 [01:35<00:15, 115.51it/s]\rPredicting:  86%|████████▌ | 10910/12660 [01:35<00:15, 115.78it/s]\rPredicting:  86%|████████▋ | 10922/12660 [01:35<00:15, 115.17it/s]\rPredicting:  86%|████████▋ | 10934/12660 [01:35<00:15, 114.79it/s]\rPredicting:  86%|████████▋ | 10946/12660 [01:35<00:14, 114.89it/s]\rPredicting:  87%|████████▋ | 10958/12660 [01:35<00:14, 115.06it/s]\rPredicting:  87%|████████▋ | 10970/12660 [01:35<00:14, 114.52it/s]\rPredicting:  87%|████████▋ | 10982/12660 [01:36<00:14, 114.91it/s]\rPredicting:  87%|████████▋ | 10994/12660 [01:36<00:14, 115.10it/s]\rPredicting:  87%|████████▋ | 11006/12660 [01:36<00:14, 114.98it/s]\rPredicting:  87%|████████▋ | 11018/12660 [01:36<00:14, 114.71it/s]\rPredicting:  87%|████████▋ | 11030/12660 [01:36<00:14, 114.52it/s]\rPredicting:  87%|████████▋ | 11042/12660 [01:36<00:14, 114.52it/s]\rPredicting:  87%|████████▋ | 11054/12660 [01:36<00:14, 114.45it/s]\rPredicting:  87%|████████▋ | 11066/12660 [01:36<00:13, 114.70it/s]\rPredicting:  88%|████████▊ | 11078/12660 [01:36<00:13, 114.55it/s]\rPredicting:  88%|████████▊ | 11090/12660 [01:36<00:13, 114.46it/s]\rPredicting:  88%|████████▊ | 11102/12660 [01:37<00:13, 114.64it/s]\rPredicting:  88%|████████▊ | 11114/12660 [01:37<00:13, 114.35it/s]\rPredicting:  88%|████████▊ | 11126/12660 [01:37<00:13, 113.53it/s]\rPredicting:  88%|████████▊ | 11138/12660 [01:37<00:13, 113.91it/s]\rPredicting:  88%|████████▊ | 11150/12660 [01:37<00:13, 114.05it/s]\rPredicting:  88%|████████▊ | 11162/12660 [01:37<00:13, 114.01it/s]\rPredicting:  88%|████████▊ | 11174/12660 [01:37<00:13, 114.08it/s]\rPredicting:  88%|████████▊ | 11186/12660 [01:37<00:12, 114.19it/s]\rPredicting:  88%|████████▊ | 11198/12660 [01:37<00:12, 113.97it/s]\rPredicting:  89%|████████▊ | 11210/12660 [01:38<00:12, 114.41it/s]\rPredicting:  89%|████████▊ | 11222/12660 [01:38<00:12, 114.67it/s]\rPredicting:  89%|████████▊ | 11234/12660 [01:38<00:12, 114.78it/s]\rPredicting:  89%|████████▉ | 11246/12660 [01:38<00:12, 114.50it/s]\rPredicting:  89%|████████▉ | 11258/12660 [01:38<00:12, 114.71it/s]\rPredicting:  89%|████████▉ | 11270/12660 [01:38<00:12, 114.84it/s]\rPredicting:  89%|████████▉ | 11282/12660 [01:38<00:12, 114.60it/s]\rPredicting:  89%|████████▉ | 11294/12660 [01:38<00:11, 114.67it/s]\rPredicting:  89%|████████▉ | 11306/12660 [01:38<00:11, 114.58it/s]\rPredicting:  89%|████████▉ | 11318/12660 [01:38<00:11, 114.86it/s]\rPredicting:  89%|████████▉ | 11330/12660 [01:39<00:11, 113.50it/s]\rPredicting:  90%|████████▉ | 11342/12660 [01:39<00:11, 112.19it/s]\rPredicting:  90%|████████▉ | 11354/12660 [01:39<00:11, 113.09it/s]\rPredicting:  90%|████████▉ | 11366/12660 [01:39<00:11, 113.47it/s]\rPredicting:  90%|████████▉ | 11378/12660 [01:39<00:11, 113.88it/s]\rPredicting:  90%|████████▉ | 11390/12660 [01:39<00:11, 114.16it/s]\rPredicting:  90%|█████████ | 11402/12660 [01:39<00:10, 114.37it/s]\rPredicting:  90%|█████████ | 11414/12660 [01:39<00:10, 114.60it/s]\rPredicting:  90%|█████████ | 11426/12660 [01:39<00:10, 115.08it/s]\rPredicting:  90%|█████████ | 11438/12660 [01:40<00:10, 114.74it/s]\rPredicting:  90%|█████████ | 11450/12660 [01:40<00:10, 114.83it/s]\rPredicting:  91%|█████████ | 11462/12660 [01:40<00:10, 114.56it/s]\rPredicting:  91%|█████████ | 11474/12660 [01:40<00:10, 114.57it/s]\rPredicting:  91%|█████████ | 11486/12660 [01:40<00:10, 114.94it/s]\rPredicting:  91%|█████████ | 11498/12660 [01:40<00:10, 115.32it/s]\rPredicting:  91%|█████████ | 11510/12660 [01:40<00:10, 114.99it/s]\rPredicting:  91%|█████████ | 11522/12660 [01:40<00:09, 115.22it/s]\rPredicting:  91%|█████████ | 11534/12660 [01:40<00:09, 115.12it/s]\rPredicting:  91%|█████████ | 11546/12660 [01:40<00:09, 115.20it/s]\rPredicting:  91%|█████████▏| 11558/12660 [01:41<00:09, 114.49it/s]\rPredicting:  91%|█████████▏| 11570/12660 [01:41<00:09, 115.03it/s]\rPredicting:  91%|█████████▏| 11582/12660 [01:41<00:09, 115.26it/s]\rPredicting:  92%|█████████▏| 11594/12660 [01:41<00:09, 115.01it/s]\rPredicting:  92%|█████████▏| 11606/12660 [01:41<00:09, 115.56it/s]\rPredicting:  92%|█████████▏| 11618/12660 [01:41<00:09, 115.28it/s]\rPredicting:  92%|█████████▏| 11630/12660 [01:41<00:09, 114.28it/s]\rPredicting:  92%|█████████▏| 11642/12660 [01:41<00:08, 113.42it/s]\rPredicting:  92%|█████████▏| 11654/12660 [01:41<00:08, 113.90it/s]\rPredicting:  92%|█████████▏| 11666/12660 [01:42<00:08, 113.91it/s]\rPredicting:  92%|█████████▏| 11678/12660 [01:42<00:08, 113.96it/s]\rPredicting:  92%|█████████▏| 11690/12660 [01:42<00:08, 113.91it/s]\rPredicting:  92%|█████████▏| 11702/12660 [01:42<00:08, 113.82it/s]\rPredicting:  93%|█████████▎| 11714/12660 [01:42<00:08, 114.25it/s]\rPredicting:  93%|█████████▎| 11726/12660 [01:42<00:08, 113.61it/s]\rPredicting:  93%|█████████▎| 11738/12660 [01:42<00:08, 113.75it/s]\rPredicting:  93%|█████████▎| 11750/12660 [01:42<00:07, 113.94it/s]\rPredicting:  93%|█████████▎| 11762/12660 [01:42<00:07, 114.20it/s]\rPredicting:  93%|█████████▎| 11774/12660 [01:42<00:07, 114.46it/s]\rPredicting:  93%|█████████▎| 11786/12660 [01:43<00:07, 114.30it/s]\rPredicting:  93%|█████████▎| 11798/12660 [01:43<00:07, 114.28it/s]\rPredicting:  93%|█████████▎| 11810/12660 [01:43<00:07, 114.60it/s]\rPredicting:  93%|█████████▎| 11822/12660 [01:43<00:07, 114.71it/s]\rPredicting:  93%|█████████▎| 11834/12660 [01:43<00:07, 114.79it/s]\rPredicting:  94%|█████████▎| 11846/12660 [01:43<00:07, 114.85it/s]\rPredicting:  94%|█████████▎| 11858/12660 [01:43<00:06, 114.58it/s]\rPredicting:  94%|█████████▍| 11870/12660 [01:43<00:06, 114.44it/s]\rPredicting:  94%|█████████▍| 11882/12660 [01:43<00:06, 114.86it/s]\rPredicting:  94%|█████████▍| 11894/12660 [01:43<00:06, 115.26it/s]\rPredicting:  94%|█████████▍| 11906/12660 [01:44<00:06, 115.11it/s]\rPredicting:  94%|█████████▍| 11918/12660 [01:44<00:06, 115.09it/s]\rPredicting:  94%|█████████▍| 11930/12660 [01:44<00:06, 115.07it/s]\rPredicting:  94%|█████████▍| 11942/12660 [01:44<00:06, 115.19it/s]\rPredicting:  94%|█████████▍| 11954/12660 [01:44<00:06, 115.23it/s]\rPredicting:  95%|█████████▍| 11966/12660 [01:44<00:06, 115.26it/s]\rPredicting:  95%|█████████▍| 11978/12660 [01:44<00:05, 115.26it/s]\rPredicting:  95%|█████████▍| 11990/12660 [01:44<00:05, 115.05it/s]\rPredicting:  95%|█████████▍| 12002/12660 [01:44<00:05, 115.03it/s]\rPredicting:  95%|█████████▍| 12014/12660 [01:45<00:05, 114.98it/s]\rPredicting:  95%|█████████▍| 12026/12660 [01:45<00:05, 114.93it/s]\rPredicting:  95%|█████████▌| 12038/12660 [01:45<00:05, 114.75it/s]\rPredicting:  95%|█████████▌| 12050/12660 [01:45<00:05, 114.66it/s]\rPredicting:  95%|█████████▌| 12062/12660 [01:45<00:05, 114.62it/s]\rPredicting:  95%|█████████▌| 12074/12660 [01:45<00:05, 114.37it/s]\rPredicting:  95%|█████████▌| 12086/12660 [01:45<00:05, 114.09it/s]\rPredicting:  96%|█████████▌| 12098/12660 [01:45<00:04, 114.40it/s]\rPredicting:  96%|█████████▌| 12110/12660 [01:45<00:04, 114.39it/s]\rPredicting:  96%|█████████▌| 12122/12660 [01:45<00:04, 113.69it/s]\rPredicting:  96%|█████████▌| 12134/12660 [01:46<00:04, 114.01it/s]\rPredicting:  96%|█████████▌| 12146/12660 [01:46<00:04, 113.72it/s]\rPredicting:  96%|█████████▌| 12158/12660 [01:46<00:04, 113.81it/s]\rPredicting:  96%|█████████▌| 12170/12660 [01:46<00:04, 113.92it/s]\rPredicting:  96%|█████████▌| 12182/12660 [01:46<00:04, 114.07it/s]\rPredicting:  96%|█████████▋| 12194/12660 [01:46<00:04, 114.35it/s]\rPredicting:  96%|█████████▋| 12206/12660 [01:46<00:03, 114.21it/s]\rPredicting:  97%|█████████▋| 12218/12660 [01:46<00:03, 114.20it/s]\rPredicting:  97%|█████████▋| 12230/12660 [01:46<00:03, 114.32it/s]\rPredicting:  97%|█████████▋| 12242/12660 [01:47<00:03, 114.42it/s]\rPredicting:  97%|█████████▋| 12254/12660 [01:47<00:03, 114.12it/s]\rPredicting:  97%|█████████▋| 12266/12660 [01:47<00:03, 113.69it/s]\rPredicting:  97%|█████████▋| 12278/12660 [01:47<00:03, 114.39it/s]\rPredicting:  97%|█████████▋| 12290/12660 [01:47<00:03, 114.21it/s]\rPredicting:  97%|█████████▋| 12302/12660 [01:47<00:03, 114.12it/s]\rPredicting:  97%|█████████▋| 12314/12660 [01:47<00:03, 114.52it/s]\rPredicting:  97%|█████████▋| 12326/12660 [01:47<00:02, 114.81it/s]\rPredicting:  97%|█████████▋| 12338/12660 [01:47<00:02, 114.49it/s]\rPredicting:  98%|█████████▊| 12350/12660 [01:47<00:02, 114.82it/s]\rPredicting:  98%|█████████▊| 12362/12660 [01:48<00:02, 114.65it/s]\rPredicting:  98%|█████████▊| 12374/12660 [01:48<00:02, 114.62it/s]\rPredicting:  98%|█████████▊| 12386/12660 [01:48<00:02, 114.78it/s]\rPredicting:  98%|█████████▊| 12398/12660 [01:48<00:02, 114.79it/s]\rPredicting:  98%|█████████▊| 12410/12660 [01:48<00:02, 114.58it/s]\rPredicting:  98%|█████████▊| 12422/12660 [01:48<00:02, 114.33it/s]\rPredicting:  98%|█████████▊| 12434/12660 [01:48<00:01, 114.26it/s]\rPredicting:  98%|█████████▊| 12446/12660 [01:48<00:01, 114.26it/s]\rPredicting:  98%|█████████▊| 12458/12660 [01:48<00:01, 114.45it/s]\rPredicting:  98%|█████████▊| 12470/12660 [01:49<00:01, 113.90it/s]\rPredicting:  99%|█████████▊| 12482/12660 [01:49<00:01, 113.74it/s]\rPredicting:  99%|█████████▊| 12494/12660 [01:49<00:01, 114.12it/s]\rPredicting:  99%|█████████▉| 12506/12660 [01:49<00:01, 114.54it/s]\rPredicting:  99%|█████████▉| 12518/12660 [01:49<00:01, 113.96it/s]\rPredicting:  99%|█████████▉| 12530/12660 [01:49<00:01, 114.06it/s]\rPredicting:  99%|█████████▉| 12542/12660 [01:49<00:01, 114.15it/s]\rPredicting:  99%|█████████▉| 12554/12660 [01:49<00:00, 113.98it/s]\rPredicting:  99%|█████████▉| 12566/12660 [01:49<00:00, 114.35it/s]\rPredicting:  99%|█████████▉| 12578/12660 [01:49<00:00, 114.80it/s]\rPredicting:  99%|█████████▉| 12590/12660 [01:50<00:00, 114.91it/s]\rPredicting: 100%|█████████▉| 12602/12660 [01:50<00:00, 114.99it/s]\rPredicting: 100%|█████████▉| 12614/12660 [01:50<00:00, 115.08it/s]\rPredicting: 100%|█████████▉| 12626/12660 [01:50<00:00, 114.95it/s]\rPredicting: 100%|█████████▉| 12638/12660 [01:50<00:00, 115.23it/s]\rPredicting: 100%|█████████▉| 12650/12660 [01:50<00:00, 115.13it/s]\rPredicting: 100%|██████████| 12660/12660 [01:50<00:00, 114.38it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_df, predict_report = bert_classifier.predict(df, \n",
    "                                                    input_columns=['Product', 'Product Category'], \n",
    "                                                    label_column='Combined_Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cfaf69e-6fdf-4887-b4d8-f73c335ae349",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">F1-Score</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Precision</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Recall</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Support</th>\n",
       "      <th colspan=\"3\" halign=\"left\">TP</th>\n",
       "      <th colspan=\"3\" halign=\"left\">FP</th>\n",
       "      <th colspan=\"3\" halign=\"left\">FN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Phase</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dev-set</th>\n",
       "      <th>test-set</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dev-set</th>\n",
       "      <th>test-set</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dev-set</th>\n",
       "      <th>test-set</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dev-set</th>\n",
       "      <th>test-set</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dev-set</th>\n",
       "      <th>test-set</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dev-set</th>\n",
       "      <th>test-set</th>\n",
       "      <th colspan=\"2\" halign=\"left\">dev-set</th>\n",
       "      <th>test-set</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Label</th>\n",
       "      <th>Label Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>Hair Care|||Combo Packs|||Combo Packs</th>\n",
       "      <td>92.71%</td>\n",
       "      <td>99.45%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>87.25%</td>\n",
       "      <td>98.9%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>98.89%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>Hair Care|||None|||None</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>Hair Care|||Conditioner|||Conditioner</th>\n",
       "      <td>95.96%</td>\n",
       "      <td>99.72%</td>\n",
       "      <td>99.75%</td>\n",
       "      <td>92.71%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.44%</td>\n",
       "      <td>99.44%</td>\n",
       "      <td>99.5%</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>199</td>\n",
       "      <td>178.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>Hair Care|||Hair Styling|||Styling Products</th>\n",
       "      <td>98.56%</td>\n",
       "      <td>99.76%</td>\n",
       "      <td>99.14%</td>\n",
       "      <td>99.03%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.14%</td>\n",
       "      <td>98.09%</td>\n",
       "      <td>99.52%</td>\n",
       "      <td>99.14%</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>232</td>\n",
       "      <td>205.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>Hair Care|||Hair Treat and Protect|||Mask</th>\n",
       "      <td>95.12%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>98.88%</td>\n",
       "      <td>90.7%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>97.78%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <th>Hair Care|||Shampoo|||Shampoo</th>\n",
       "      <td>92.11%</td>\n",
       "      <td>99.84%</td>\n",
       "      <td>98.84%</td>\n",
       "      <td>85.37%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>99.68%</td>\n",
       "      <td>97.71%</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>350</td>\n",
       "      <td>315.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>342.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>Hair Care|||Shampoo|||Therapeutic</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>98.63%</td>\n",
       "      <td>98.77%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>97.3%</td>\n",
       "      <td>97.56%</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>Hair Care|||Hair Treat and Protect|||Leave In Conditioner</th>\n",
       "      <td>68.18%</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>98.41%</td>\n",
       "      <td>93.75%</td>\n",
       "      <td>96.55%</td>\n",
       "      <td>96.88%</td>\n",
       "      <td>53.57%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>Hair Care|||Hair Treat and Protect|||Oil</th>\n",
       "      <td>91.18%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>97.81%</td>\n",
       "      <td>83.78%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>98.53%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>97.1%</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>62.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>Hair Care|||Hair Regrowth|||Hair Regrowth</th>\n",
       "      <td>97.14%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>95.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>95.0%</td>\n",
       "      <td>94.44%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>95.0%</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>Hair Care|||Hair Styling|||Hair Spray</th>\n",
       "      <td>95.24%</td>\n",
       "      <td>95.74%</td>\n",
       "      <td>94.79%</td>\n",
       "      <td>90.91%</td>\n",
       "      <td>91.84%</td>\n",
       "      <td>90.09%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>90.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>Hair Care|||Shampoo|||Multi Benefit</th>\n",
       "      <td>11.76%</td>\n",
       "      <td>98.46%</td>\n",
       "      <td>90.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>96.97%</td>\n",
       "      <td>81.82%</td>\n",
       "      <td>6.25%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>Hair Care|||Vitamins &amp; Supplements|||Vitamins &amp; Supplements</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>85.71%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>75.0%</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>Hair Care|||Hair Treat and Protect|||Treatment</th>\n",
       "      <td>78.79%</td>\n",
       "      <td>82.35%</td>\n",
       "      <td>83.33%</td>\n",
       "      <td>68.42%</td>\n",
       "      <td>70.0%</td>\n",
       "      <td>71.43%</td>\n",
       "      <td>92.86%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>13.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>Hair Care|||Hair Styling|||Styling Protection</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>Hair Care|||Hair Treat and Protect|||Serum</th>\n",
       "      <td>0.0%</td>\n",
       "      <td>28.57%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>16.67%</td>\n",
       "      <td>0.0%</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <th>macro avg</th>\n",
       "      <td>57.3%</td>\n",
       "      <td>87.55%</td>\n",
       "      <td>83.78%</td>\n",
       "      <td>62.0%</td>\n",
       "      <td>90.89%</td>\n",
       "      <td>83.17%</td>\n",
       "      <td>58.97%</td>\n",
       "      <td>88.29%</td>\n",
       "      <td>85.06%</td>\n",
       "      <td>1140</td>\n",
       "      <td>1140</td>\n",
       "      <td>1266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <th>weighted avg</th>\n",
       "      <td>86.14%</td>\n",
       "      <td>98.06%</td>\n",
       "      <td>97.04%</td>\n",
       "      <td>85.55%</td>\n",
       "      <td>98.03%</td>\n",
       "      <td>96.71%</td>\n",
       "      <td>89.91%</td>\n",
       "      <td>98.51%</td>\n",
       "      <td>97.55%</td>\n",
       "      <td>1140</td>\n",
       "      <td>1140</td>\n",
       "      <td>1266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                F1-Score  ...       FN\n",
       "Phase                                                            dev-set  ... test-set\n",
       "Epoch                                                                  1  ...        3\n",
       "Label        Label Name                                                   ...         \n",
       "0            Hair Care|||Combo Packs|||Combo Packs                92.71%  ...      0.0\n",
       "11           Hair Care|||None|||None                                0.0%  ...      0.0\n",
       "1            Hair Care|||Conditioner|||Conditioner                95.96%  ...      1.0\n",
       "4            Hair Care|||Hair Styling|||Styling Products          98.56%  ...      2.0\n",
       "7            Hair Care|||Hair Treat and Protect|||Mask            95.12%  ...      0.0\n",
       "13           Hair Care|||Shampoo|||Shampoo                        92.11%  ...      8.0\n",
       "14           Hair Care|||Shampoo|||Therapeutic                      0.0%  ...      1.0\n",
       "6            Hair Care|||Hair Treat and Protect|||Leave In C...   68.18%  ...      0.0\n",
       "8            Hair Care|||Hair Treat and Protect|||Oil             91.18%  ...      2.0\n",
       "2            Hair Care|||Hair Regrowth|||Hair Regrowth            97.14%  ...      1.0\n",
       "3            Hair Care|||Hair Styling|||Hair Spray                95.24%  ...      0.0\n",
       "12           Hair Care|||Shampoo|||Multi Benefit                  11.76%  ...      0.0\n",
       "15           Hair Care|||Vitamins & Supplements|||Vitamins &...     0.0%  ...      1.0\n",
       "10           Hair Care|||Hair Treat and Protect|||Treatment       78.79%  ...      0.0\n",
       "5            Hair Care|||Hair Styling|||Styling Protection          0.0%  ...      9.0\n",
       "9            Hair Care|||Hair Treat and Protect|||Serum             0.0%  ...      6.0\n",
       "macro avg    macro avg                                             57.3%  ...      NaN\n",
       "weighted avg weighted avg                                         86.14%  ...      NaN\n",
       "\n",
       "[18 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d6ba2b-38f4-404a-8169-0ace298c29ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_name</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hair Care|||Hair Styling|||Styling Products</td>\n",
       "      <td>99.74%</td>\n",
       "      <td>99.66%</td>\n",
       "      <td>99.7%</td>\n",
       "      <td>2319.000000</td>\n",
       "      <td>2311.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hair Care|||Conditioner|||Conditioner</td>\n",
       "      <td>99.7%</td>\n",
       "      <td>99.6%</td>\n",
       "      <td>99.65%</td>\n",
       "      <td>1991.000000</td>\n",
       "      <td>1983.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hair Care|||Hair Treat and Protect|||Mask</td>\n",
       "      <td>99.32%</td>\n",
       "      <td>99.77%</td>\n",
       "      <td>99.54%</td>\n",
       "      <td>438.000000</td>\n",
       "      <td>437.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hair Care|||None|||None</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>98.98%</td>\n",
       "      <td>99.49%</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hair Care|||Hair Treat and Protect|||Oil</td>\n",
       "      <td>99.42%</td>\n",
       "      <td>99.42%</td>\n",
       "      <td>99.42%</td>\n",
       "      <td>688.000000</td>\n",
       "      <td>684.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hair Care|||Shampoo|||Shampoo</td>\n",
       "      <td>99.83%</td>\n",
       "      <td>99.0%</td>\n",
       "      <td>99.41%</td>\n",
       "      <td>3499.000000</td>\n",
       "      <td>3464.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hair Care|||Combo Packs|||Combo Packs</td>\n",
       "      <td>99.5%</td>\n",
       "      <td>99.3%</td>\n",
       "      <td>99.4%</td>\n",
       "      <td>997.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Hair Care|||Shampoo|||Therapeutic</td>\n",
       "      <td>99.02%</td>\n",
       "      <td>98.54%</td>\n",
       "      <td>98.78%</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>405.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hair Care|||Hair Treat and Protect|||Leave In ...</td>\n",
       "      <td>96.88%</td>\n",
       "      <td>99.04%</td>\n",
       "      <td>97.95%</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hair Care|||Vitamins &amp; Supplements|||Vitamins ...</td>\n",
       "      <td>97.56%</td>\n",
       "      <td>97.56%</td>\n",
       "      <td>97.56%</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hair Care|||Hair Regrowth|||Hair Regrowth</td>\n",
       "      <td>95.12%</td>\n",
       "      <td>98.48%</td>\n",
       "      <td>96.77%</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hair Care|||Hair Styling|||Hair Spray</td>\n",
       "      <td>92.69%</td>\n",
       "      <td>99.7%</td>\n",
       "      <td>96.07%</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>1001.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hair Care|||Shampoo|||Multi Benefit</td>\n",
       "      <td>91.54%</td>\n",
       "      <td>99.44%</td>\n",
       "      <td>95.33%</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hair Care|||Hair Treat and Protect|||Treatment</td>\n",
       "      <td>73.56%</td>\n",
       "      <td>98.71%</td>\n",
       "      <td>84.3%</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hair Care|||Hair Treat and Protect|||Serum</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>9.84%</td>\n",
       "      <td>17.91%</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hair Care|||Hair Styling|||Styling Protection</td>\n",
       "      <td>100.0%</td>\n",
       "      <td>5.68%</td>\n",
       "      <td>10.75%</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>0.982464</td>\n",
       "      <td>0.982464</td>\n",
       "      <td>0.982464</td>\n",
       "      <td>0.982464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>macro avg</td>\n",
       "      <td>96.49%</td>\n",
       "      <td>87.67%</td>\n",
       "      <td>87.0%</td>\n",
       "      <td>12660.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>weighted avg</td>\n",
       "      <td>98.43%</td>\n",
       "      <td>98.25%</td>\n",
       "      <td>97.83%</td>\n",
       "      <td>12660.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     label_name  ...         FN\n",
       "4                   Hair Care|||Hair Styling|||Styling Products  ...   8.000000\n",
       "1                         Hair Care|||Conditioner|||Conditioner  ...   8.000000\n",
       "7                     Hair Care|||Hair Treat and Protect|||Mask  ...   1.000000\n",
       "11                                      Hair Care|||None|||None  ...   1.000000\n",
       "8                      Hair Care|||Hair Treat and Protect|||Oil  ...   4.000000\n",
       "13                                Hair Care|||Shampoo|||Shampoo  ...  35.000000\n",
       "0                         Hair Care|||Combo Packs|||Combo Packs  ...   7.000000\n",
       "14                            Hair Care|||Shampoo|||Therapeutic  ...   6.000000\n",
       "6             Hair Care|||Hair Treat and Protect|||Leave In ...  ...   3.000000\n",
       "15            Hair Care|||Vitamins & Supplements|||Vitamins ...  ...   1.000000\n",
       "2                     Hair Care|||Hair Regrowth|||Hair Regrowth  ...   3.000000\n",
       "3                         Hair Care|||Hair Styling|||Hair Spray  ...   3.000000\n",
       "12                          Hair Care|||Shampoo|||Multi Benefit  ...   2.000000\n",
       "10               Hair Care|||Hair Treat and Protect|||Treatment  ...   2.000000\n",
       "9                    Hair Care|||Hair Treat and Protect|||Serum  ...  55.000000\n",
       "5                 Hair Care|||Hair Styling|||Styling Protection  ...  83.000000\n",
       "accuracy                                               accuracy  ...   0.982464\n",
       "macro avg                                             macro avg  ...        NaN\n",
       "weighted avg                                       weighted avg  ...        NaN\n",
       "\n",
       "[19 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_report"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RoBERTa - Text Classifier Framework",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
